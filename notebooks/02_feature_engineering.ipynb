{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering\n",
        "## Agriculture Crop Production Dataset\n",
        "\n",
        "This notebook focuses on feature engineering, creating new features from existing data to improve model performance.\n",
        "\n",
        "**Objectives:**\n",
        "- Create temporal features\n",
        "- Generate derived features\n",
        "- Encode categorical variables\n",
        "- Perform feature selection\n",
        "- Prepare data for model training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append('../src')\n",
        "from utils.data_loader import load_data, preprocess_data\n",
        "from utils.preprocessing import FeatureEncoder, prepare_model_features\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Preprocess Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = load_data()\n",
        "print(f\"Original data shape: {df.shape}\")\n",
        "\n",
        "# Preprocess data (handles missing values, outliers)\n",
        "df_processed = preprocess_data(df)\n",
        "print(f\"Processed data shape: {df_processed.shape}\")\n",
        "\n",
        "# Display first few rows\n",
        "df_processed.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Temporal Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create temporal features\n",
        "if 'Year' in df_processed.columns:\n",
        "    # Year squared (captures non-linear trends)\n",
        "    df_processed['Year_Squared'] = df_processed['Year'] ** 2\n",
        "    \n",
        "    # Year normalized (0-1 scale)\n",
        "    min_year = df_processed['Year'].min()\n",
        "    max_year = df_processed['Year'].max()\n",
        "    df_processed['Year_Normalized'] = (df_processed['Year'] - min_year) / (max_year - min_year)\n",
        "    \n",
        "    # Years since start\n",
        "    df_processed['Years_Since_Start'] = df_processed['Year'] - df_processed['Year'].min()\n",
        "    \n",
        "    print(\"Temporal features created:\")\n",
        "    print(f\"  - Year_Squared: {df_processed['Year_Squared'].min():.0f} to {df_processed['Year_Squared'].max():.0f}\")\n",
        "    print(f\"  - Year_Normalized: {df_processed['Year_Normalized'].min():.2f} to {df_processed['Year_Normalized'].max():.2f}\")\n",
        "    print(f\"  - Years_Since_Start: {df_processed['Years_Since_Start'].min():.0f} to {df_processed['Years_Since_Start'].max():.0f}\")\n",
        "    \n",
        "    # Visualize temporal features\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    \n",
        "    axes[0].scatter(df_processed['Year'], df_processed['Year_Squared'], alpha=0.6)\n",
        "    axes[0].set_title('Year vs Year Squared', fontweight='bold')\n",
        "    axes[0].set_xlabel('Year')\n",
        "    axes[0].set_ylabel('Year Squared')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    axes[1].hist(df_processed['Year_Normalized'], bins=20, edgecolor='black')\n",
        "    axes[1].set_title('Year Normalized Distribution', fontweight='bold')\n",
        "    axes[1].set_xlabel('Year Normalized')\n",
        "    axes[1].set_ylabel('Frequency')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    axes[2].hist(df_processed['Years_Since_Start'], bins=20, edgecolor='black', color='green')\n",
        "    axes[2].set_title('Years Since Start Distribution', fontweight='bold')\n",
        "    axes[2].set_xlabel('Years Since Start')\n",
        "    axes[2].set_ylabel('Frequency')\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"⚠️ 'Year' column not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Derived Features (Cost and Production Related)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create derived features\n",
        "if all(col in df_processed.columns for col in ['Cost', 'Production', 'Quantity']):\n",
        "    # Cost per unit production\n",
        "    df_processed['Cost_per_Unit'] = df_processed['Cost'] / (df_processed['Production'] + 1e-6)  # Avoid division by zero\n",
        "    \n",
        "    # Production per cost (efficiency metric)\n",
        "    df_processed['Production_per_Cost'] = df_processed['Production'] / (df_processed['Cost'] + 1e-6)\n",
        "    \n",
        "    # Cost per hectare (if area data available, otherwise use yield as proxy)\n",
        "    df_processed['Cost_per_Hectare'] = df_processed['Cost'] / (df_processed['Quantity'] + 1e-6)\n",
        "    \n",
        "    # Profitability indicator (higher production with lower cost)\n",
        "    df_processed['Profitability_Indicator'] = (df_processed['Production'] / df_processed['Production'].max()) / (df_processed['Cost'] / df_processed['Cost'].max() + 1e-6)\n",
        "    \n",
        "    print(\"Derived features created:\")\n",
        "    print(f\"  - Cost_per_Unit: {df_processed['Cost_per_Unit'].min():.2f} to {df_processed['Cost_per_Unit'].max():.2f}\")\n",
        "    print(f\"  - Production_per_Cost: {df_processed['Production_per_Cost'].min():.2f} to {df_processed['Production_per_Cost'].max():.2f}\")\n",
        "    print(f\"  - Cost_per_Hectare: {df_processed['Cost_per_Hectare'].min():.2f} to {df_processed['Cost_per_Hectare'].max():.2f}\")\n",
        "    \n",
        "    # Visualize derived features\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    df_processed['Cost_per_Unit'].hist(bins=50, ax=axes[0, 0], edgecolor='black')\n",
        "    axes[0, 0].set_title('Cost per Unit Distribution', fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Cost per Unit')\n",
        "    axes[0, 0].set_ylabel('Frequency')\n",
        "    \n",
        "    df_processed['Production_per_Cost'].hist(bins=50, ax=axes[0, 1], edgecolor='black', color='green')\n",
        "    axes[0, 1].set_title('Production per Cost Distribution', fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Production per Cost')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    \n",
        "    axes[1, 0].scatter(df_processed['Cost'], df_processed['Production'], alpha=0.5)\n",
        "    axes[1, 0].set_title('Cost vs Production', fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Cost')\n",
        "    axes[1, 0].set_ylabel('Production')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    df_processed['Profitability_Indicator'].hist(bins=50, ax=axes[1, 1], edgecolor='black', color='orange')\n",
        "    axes[1, 1].set_title('Profitability Indicator Distribution', fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Profitability Indicator')\n",
        "    axes[1, 1].set_ylabel('Frequency')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"⚠️ Required columns (Cost, Production, Quantity) not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Categorical Encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode categorical variables\n",
        "categorical_cols = ['Crop', 'State', 'Season'] if all(col in df_processed.columns for col in ['Crop', 'State', 'Season']) else []\n",
        "\n",
        "if categorical_cols:\n",
        "    # Create FeatureEncoder\n",
        "    encoder = FeatureEncoder()\n",
        "    \n",
        "    # Fit encoder (requires categorical_cols parameter)\n",
        "    encoder.fit(df_processed, categorical_cols)\n",
        "    \n",
        "    # Transform data (requires categorical_cols parameter)\n",
        "    df_encoded = encoder.transform(df_processed.copy(), categorical_cols)\n",
        "    \n",
        "    print(\"Categorical encoding completed:\")\n",
        "    for col in categorical_cols:\n",
        "        if col in df_processed.columns:\n",
        "            print(f\"  - {col}: {df_processed[col].nunique()} unique values\")\n",
        "            print(f\"    Encoded values: {df_encoded[col].min()} to {df_encoded[col].max()}\")\n",
        "    \n",
        "    # Show encoding mapping\n",
        "    print(\"\\nEncoding mappings (sample):\")\n",
        "    for col in categorical_cols[:2]:  # Show first 2 columns\n",
        "        if col in encoder.label_encoders:\n",
        "            le = encoder.label_encoders[col]\n",
        "            print(f\"\\n{col} encoding (first 5):\")\n",
        "            for i, label in enumerate(le.classes_[:5]):\n",
        "                print(f\"  {label} -> {i}\")\n",
        "    \n",
        "    # Visualize encoded distributions\n",
        "    fig, axes = plt.subplots(1, len(categorical_cols), figsize=(15, 5))\n",
        "    if len(categorical_cols) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for idx, col in enumerate(categorical_cols):\n",
        "        if col in df_encoded.columns:\n",
        "            df_encoded[col].value_counts().sort_index().head(20).plot(kind='bar', ax=axes[idx])\n",
        "            axes[idx].set_title(f'{col} Encoded Distribution', fontweight='bold')\n",
        "            axes[idx].set_xlabel(f'{col} (Encoded)')\n",
        "            axes[idx].set_ylabel('Frequency')\n",
        "            axes[idx].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    df_processed = df_encoded\n",
        "else:\n",
        "    print(\"⚠️ Categorical columns not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Selection and Importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature selection using correlation and variance\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Prepare features and target\n",
        "target_col = 'Quantity' if 'Quantity' in df_processed.columns else df_processed.select_dtypes(include=[np.number]).columns[-1]\n",
        "\n",
        "# Select numerical features\n",
        "feature_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
        "feature_cols = [col for col in feature_cols if col != target_col and 'Quantity' not in col]\n",
        "\n",
        "X = df_processed[feature_cols].fillna(0)\n",
        "y = df_processed[target_col].fillna(0)\n",
        "\n",
        "print(f\"Target variable: {target_col}\")\n",
        "print(f\"Number of features: {len(feature_cols)}\")\n",
        "\n",
        "# Calculate feature importance using Random Forest\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_cols,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance.head(10).to_string(index=False))\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_features = feature_importance.head(15)\n",
        "sns.barplot(data=top_features, y='Feature', x='Importance', palette='viridis')\n",
        "plt.title('Top 15 Feature Importance (Random Forest)', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Correlation Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze feature correlations\n",
        "numerical_features = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
        "correlation_matrix = df_processed[numerical_features].corr()\n",
        "\n",
        "# Visualize correlation matrix\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(correlation_matrix, annot=False, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find highly correlated features (potential redundancy)\n",
        "high_corr_pairs = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        corr_value = correlation_matrix.iloc[i, j]\n",
        "        if abs(corr_value) > 0.8:  # High correlation threshold\n",
        "            high_corr_pairs.append({\n",
        "                'Feature 1': correlation_matrix.columns[i],\n",
        "                'Feature 2': correlation_matrix.columns[j],\n",
        "                'Correlation': corr_value\n",
        "            })\n",
        "\n",
        "if high_corr_pairs:\n",
        "    print(\"\\nHighly Correlated Feature Pairs (|r| > 0.8):\")\n",
        "    print(pd.DataFrame(high_corr_pairs).to_string(index=False))\n",
        "else:\n",
        "    print(\"\\n✅ No highly correlated feature pairs found (|r| > 0.8)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Final Feature Set Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare final feature set for modeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select final features (exclude target and identifier columns)\n",
        "exclude_cols = [target_col, 'Year', 'Season_Duration', 'Recommended_Zone', 'Variety', 'Unit']\n",
        "final_features = [col for col in feature_cols if col not in exclude_cols]\n",
        "\n",
        "X_final = df_processed[final_features].fillna(0)\n",
        "y_final = df_processed[target_col].fillna(0)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_final, y_final, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Final Feature Set:\")\n",
        "print(f\"  Total features: {len(final_features)}\")\n",
        "print(f\"  Training samples: {len(X_train)}\")\n",
        "print(f\"  Test samples: {len(X_test)}\")\n",
        "print(f\"\\nFeatures included:\")\n",
        "for i, feat in enumerate(final_features, 1):\n",
        "    print(f\"  {i}. {feat}\")\n",
        "\n",
        "# Optional: Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"\\n✅ Features scaled and ready for modeling!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Next Steps\n",
        "\n",
        "### Features Created:\n",
        "1. **Temporal Features:**\n",
        "   - Year_Squared\n",
        "   - Year_Normalized\n",
        "   - Years_Since_Start\n",
        "\n",
        "2. **Derived Features:**\n",
        "   - Cost_per_Unit\n",
        "   - Production_per_Cost\n",
        "   - Cost_per_Hectare\n",
        "   - Profitability_Indicator\n",
        "\n",
        "3. **Encoded Features:**\n",
        "   - Crop_encoded\n",
        "   - State_encoded\n",
        "   - Season_encoded\n",
        "\n",
        "### Key Insights:\n",
        "- [Add insights from feature importance analysis]\n",
        "- [Add observations about feature correlations]\n",
        "- [Add recommendations for feature selection]\n",
        "\n",
        "### Next Steps:\n",
        "- Proceed to model training (Week 3)\n",
        "- Use prepared features for Random Forest and XGBoost\n",
        "- Consider feature selection based on importance scores\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
