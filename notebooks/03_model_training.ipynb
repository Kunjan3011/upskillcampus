{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training and Evaluation\n",
        "## Agriculture Crop Production Prediction\n",
        "\n",
        "This notebook focuses on training and evaluating machine learning models for crop yield prediction.\n",
        "\n",
        "**Models to Train:**\n",
        "- Random Forest Regressor\n",
        "- XGBoost Regressor\n",
        "- Model Comparison and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML Libraries\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append('../src')\n",
        "from utils.data_loader import load_data, preprocess_data\n",
        "from utils.preprocessing import prepare_model_features\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "df = load_data()\n",
        "df_processed = preprocess_data(df)\n",
        "\n",
        "# Prepare features for modeling\n",
        "X, y, encoder = prepare_model_features(df_processed, target_col='Quantity')\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"\\nFeature names: {list(X.columns) if hasattr(X, 'columns') else 'Array format'}\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"Target range: {y.min():.2f} to {y.max():.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Random Forest Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Random Forest Model\n",
        "print(\"Training Random Forest Regressor...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=20,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred_rf = rf_model.predict(X_train)\n",
        "y_test_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "train_rmse_rf = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))\n",
        "test_rmse_rf = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))\n",
        "train_mae_rf = mean_absolute_error(y_train, y_train_pred_rf)\n",
        "test_mae_rf = mean_absolute_error(y_test, y_test_pred_rf)\n",
        "train_r2_rf = r2_score(y_train, y_train_pred_rf)\n",
        "test_r2_rf = r2_score(y_test, y_test_pred_rf)\n",
        "\n",
        "print(\"Random Forest Results:\")\n",
        "print(f\"  Training RMSE: {train_rmse_rf:.4f}\")\n",
        "print(f\"  Test RMSE: {test_rmse_rf:.4f}\")\n",
        "print(f\"  Training MAE: {train_mae_rf:.4f}\")\n",
        "print(f\"  Test MAE: {test_mae_rf:.4f}\")\n",
        "print(f\"  Training R²: {train_r2_rf:.4f}\")\n",
        "print(f\"  Test R²: {test_r2_rf:.4f}\")\n",
        "\n",
        "# Feature importance\n",
        "feature_importance_rf = pd.DataFrame({\n",
        "    'Feature': X.columns if hasattr(X, 'columns') else [f'Feature_{i}' for i in range(X.shape[1])],\n",
        "    'Importance': rf_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features (Random Forest):\")\n",
        "print(feature_importance_rf.head(10).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. XGBoost Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train XGBoost Model\n",
        "print(\"Training XGBoost Regressor...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred_xgb = xgb_model.predict(X_train)\n",
        "y_test_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "train_rmse_xgb = np.sqrt(mean_squared_error(y_train, y_train_pred_xgb))\n",
        "test_rmse_xgb = np.sqrt(mean_squared_error(y_test, y_test_pred_xgb))\n",
        "train_mae_xgb = mean_absolute_error(y_train, y_train_pred_xgb)\n",
        "test_mae_xgb = mean_absolute_error(y_test, y_test_pred_xgb)\n",
        "train_r2_xgb = r2_score(y_train, y_train_pred_xgb)\n",
        "test_r2_xgb = r2_score(y_test, y_test_pred_xgb)\n",
        "\n",
        "print(\"XGBoost Results:\")\n",
        "print(f\"  Training RMSE: {train_rmse_xgb:.4f}\")\n",
        "print(f\"  Test RMSE: {test_rmse_xgb:.4f}\")\n",
        "print(f\"  Training MAE: {train_mae_xgb:.4f}\")\n",
        "print(f\"  Test MAE: {test_mae_xgb:.4f}\")\n",
        "print(f\"  Training R²: {train_r2_xgb:.4f}\")\n",
        "print(f\"  Test R²: {test_r2_xgb:.4f}\")\n",
        "\n",
        "# Feature importance\n",
        "feature_importance_xgb = pd.DataFrame({\n",
        "    'Feature': X.columns if hasattr(X, 'columns') else [f'Feature_{i}' for i in range(X.shape[1])],\n",
        "    'Importance': xgb_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features (XGBoost):\")\n",
        "print(feature_importance_xgb.head(10).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare models\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': ['Random Forest', 'XGBoost'],\n",
        "    'Train RMSE': [train_rmse_rf, train_rmse_xgb],\n",
        "    'Test RMSE': [test_rmse_rf, test_rmse_xgb],\n",
        "    'Train MAE': [train_mae_rf, train_mae_xgb],\n",
        "    'Test MAE': [test_mae_rf, test_mae_xgb],\n",
        "    'Train R²': [train_r2_rf, train_r2_xgb],\n",
        "    'Test R²': [test_r2_rf, test_r2_xgb]\n",
        "})\n",
        "\n",
        "print(\"Model Comparison:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# RMSE comparison\n",
        "axes[0, 0].bar(['Random Forest', 'XGBoost'], [test_rmse_rf, test_rmse_xgb], \n",
        "               color=['skyblue', 'lightgreen'])\n",
        "axes[0, 0].set_title('Test RMSE Comparison', fontweight='bold')\n",
        "axes[0, 0].set_ylabel('RMSE')\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# MAE comparison\n",
        "axes[0, 1].bar(['Random Forest', 'XGBoost'], [test_mae_rf, test_mae_xgb],\n",
        "               color=['skyblue', 'lightgreen'])\n",
        "axes[0, 1].set_title('Test MAE Comparison', fontweight='bold')\n",
        "axes[0, 1].set_ylabel('MAE')\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# R² comparison\n",
        "axes[1, 0].bar(['Random Forest', 'XGBoost'], [test_r2_rf, test_r2_xgb],\n",
        "               color=['skyblue', 'lightgreen'])\n",
        "axes[1, 0].set_title('Test R² Comparison', fontweight='bold')\n",
        "axes[1, 0].set_ylabel('R² Score')\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Prediction vs Actual (Best model)\n",
        "best_model_name = 'XGBoost' if test_r2_xgb > test_r2_rf else 'Random Forest'\n",
        "best_predictions = y_test_pred_xgb if test_r2_xgb > test_r2_rf else y_test_pred_rf\n",
        "\n",
        "axes[1, 1].scatter(y_test, best_predictions, alpha=0.5)\n",
        "axes[1, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "axes[1, 1].set_title(f'Predicted vs Actual ({best_model_name})', fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Actual Values')\n",
        "axes[1, 1].set_ylabel('Predicted Values')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n✅ Best Model: {best_model_name} (R² = {max(test_r2_rf, test_r2_xgb):.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Cross-Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform cross-validation\n",
        "print(\"Performing 5-fold Cross-Validation...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Random Forest CV\n",
        "rf_cv_scores = cross_val_score(rf_model, X_train, y_train, \n",
        "                                cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "rf_cv_rmse = np.sqrt(-rf_cv_scores)\n",
        "rf_cv_r2 = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='r2', n_jobs=-1)\n",
        "\n",
        "# XGBoost CV\n",
        "xgb_cv_scores = cross_val_score(xgb_model, X_train, y_train,\n",
        "                                 cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "xgb_cv_rmse = np.sqrt(-xgb_cv_scores)\n",
        "xgb_cv_r2 = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='r2', n_jobs=-1)\n",
        "\n",
        "print(\"Random Forest Cross-Validation:\")\n",
        "print(f\"  RMSE: {rf_cv_rmse.mean():.4f} (+/- {rf_cv_rmse.std() * 2:.4f})\")\n",
        "print(f\"  R²: {rf_cv_r2.mean():.4f} (+/- {rf_cv_r2.std() * 2:.4f})\")\n",
        "\n",
        "print(\"\\nXGBoost Cross-Validation:\")\n",
        "print(f\"  RMSE: {xgb_cv_rmse.mean():.4f} (+/- {xgb_cv_rmse.std() * 2:.4f})\")\n",
        "print(f\"  R²: {xgb_cv_r2.mean():.4f} (+/- {xgb_cv_r2.std() * 2:.4f})\")\n",
        "\n",
        "# Visualize CV results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "axes[0].boxplot([rf_cv_rmse, xgb_cv_rmse], labels=['Random Forest', 'XGBoost'])\n",
        "axes[0].set_title('Cross-Validation RMSE Distribution', fontweight='bold')\n",
        "axes[0].set_ylabel('RMSE')\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "axes[1].boxplot([rf_cv_r2, xgb_cv_r2], labels=['Random Forest', 'XGBoost'])\n",
        "axes[1].set_title('Cross-Validation R² Distribution', fontweight='bold')\n",
        "axes[1].set_ylabel('R² Score')\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save trained models\n",
        "models_dir = Path(\"../models/saved_models\")\n",
        "models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save Random Forest\n",
        "joblib.dump(rf_model, models_dir / \"random_forest_model.joblib\")\n",
        "print(\"✅ Random Forest model saved\")\n",
        "\n",
        "# Save XGBoost\n",
        "joblib.dump(xgb_model, models_dir / \"xgboost_model.joblib\")\n",
        "print(\"✅ XGBoost model saved\")\n",
        "\n",
        "# Save encoder\n",
        "joblib.dump(encoder, models_dir / \"feature_encoder.joblib\")\n",
        "print(\"✅ Feature encoder saved\")\n",
        "\n",
        "print(f\"\\nModels saved to: {models_dir.absolute()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary\n",
        "\n",
        "### Model Performance Summary:\n",
        "- **Best Model:** [Random Forest / XGBoost]\n",
        "- **Test R² Score:** [Value]\n",
        "- **Test RMSE:** [Value]\n",
        "- **Test MAE:** [Value]\n",
        "\n",
        "### Key Insights:\n",
        "- [Add insights about model performance]\n",
        "- [Add observations about feature importance]\n",
        "- [Add recommendations for model improvement]\n",
        "\n",
        "### Next Steps:\n",
        "- Proceed to ensemble modeling\n",
        "- Implement time-series models (ARIMA, Prophet)\n",
        "- Deploy models to API\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
